#!/bin/bash
#SBATCH --job-name=centrifuge
#SBATCH --output=logs/centrifuge_db%A.out
#SBATCH --error=logs/centrifuge_db%A.err
#SBATCH --partition=epyc2
#SBATCH --qos=job_cpu
#SBATCH --time=2-00:00:00
#SBATCH --cpus-per-task=16
#SBATCH --mem=512G
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=jazmin.valerianosaenz@students.unibe.ch

#This script builds a centrifuge db comparable to kraken's "standard database"

module load Anaconda3
eval "$(conda shell.bash hook)"
conda activate centrifuge_env

# Paths to Kraken files
KRAKEN_DB="databases/kraken2_std/"
KRAKEN_MAPFILE="$KRAKEN_DB/seqid2taxid.map"
TAX_TREE="$KRAKEN_DB/taxonomy/nodes.dmp"
TAX_NAMES="$KRAKEN_DB/taxonomy/names.dmp"
SEQUENCES_DIR="$KRAKEN_DB/library"

# Centrifuge files
CENTRIFUGE_DB="databases/centrifuge_std"
CONVERSION_TABLE="$CENTRIFUGE_DB/seqid2taxid_centrifuge.map"

mkdir -p "$CENTRIFUGE_DB"

# Reformat the mapping file generated by kraken 
awk -F'\\||\t' '{print $3 "\t" $NF}' $KRAKEN_MAPFILE > $CONVERSION_TABLE

# Merge the sequences downloaded for kraken's standard db into one file (archaea, bacteria, human, plasmid, viral)
cat $SEQUENCES_DIR/*/library.fna > $CENTRIFUGE_DB/centrifuge_sequences.fna

# Clean the sequences headers 
awk '/^>/ {split($1,a,"|"); printf(">%s", a[3]); for(i=4; i<=NF; i++) printf(" %s", $i); printf("\n"); next} {print}' \
"$CENTRIFUGE_DB/centrifuge_sequences.fna" > tmp.fna && mv tmp.fna "$CENTRIFUGE_DB/centrifuge_sequences.fna"

#Count check
grep -c "^>" "$CENTRIFUGE_DB/centrifuge_sequences.fna" > "$CENTRIFUGE_DB/num_sequences.txt"

centrifuge-build \
  -p 16 \
  --conversion-table $CONVERSION_TABLE \
  --taxonomy-tree $TAX_TREE \
  --name-table $TAX_NAMES \
  $CENTRIFUGE_DB/centrifuge_sequences.fna \
  $CENTRIFUGE_DB

conda deactivate