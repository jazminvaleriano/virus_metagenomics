#!/bin/bash
#SBATCH --job-name=centrifuge
#SBATCH --output=logs/centrifuge_db%A.out
#SBATCH --error=logs/centrifuge_db%A.err
#SBATCH --partition=epyc2
#SBATCH --qos=job_cpu
#SBATCH --time=2-12:00:00
#SBATCH --cpus-per-task=16
#SBATCH --mem=512G
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=jazmin.valerianosaenz@students.unibe.ch

# This script builds a centrifuge db comparable to Kraken's "standard database"

module load Anaconda3
eval "$(conda shell.bash hook)"
conda activate centrifuge_env

# Paths to Kraken files
KRAKEN_DB="databases/kraken2_std"
KRAKEN_MAPFILE="$KRAKEN_DB/seqid2taxid.map"
TAX_TREE="$KRAKEN_DB/taxonomy/nodes.dmp"
TAX_NAMES="$KRAKEN_DB/taxonomy/names.dmp"
SEQUENCES_DIR="$KRAKEN_DB/library"

# Centrifuge files
CENTRIFUGE_DB="databases/centrifuge_std"
CONVERSION_TABLE="$CENTRIFUGE_DB/seqid2taxid_centrifuge.map"

mkdir -p "$CENTRIFUGE_DB"

# Reformat the mapping file generated by Kraken
awk 'BEGIN { FS = "[\t|]" }
{
  if ($1 ~ /^kraken:taxid/) {
    print $3 "\t" $NF
  } else if (NF == 2 && $1 ~ /^[A-Z_0-9.]+$/ && $2 ~ /^[0-9]+$/) {
    print $1 "\t" $2
  }
}' "$KRAKEN_MAPFILE" > "$CONVERSION_TABLE"

# Merge the sequences downloaded for Kraken's standard DB
cat "$SEQUENCES_DIR"/*/library.fna > "$CENTRIFUGE_DB/centrifuge_sequences.fna"

# Clean the sequence headers (preserve accession and description)
awk '
/^>/ {
  if ($0 ~ /^>kraken:taxid\|[0-9]+\|[^ ]+/) {
    match($0, />kraken:taxid\|[0-9]+\|([^ ]+)/, m);
    if (m[1] != "") {
      sub(/^>kraken:taxid\|[0-9]+\|[^ ]+/, ">" m[1]);
    }
  }
  print;
  next;
}
{ print }
' "$CENTRIFUGE_DB/centrifuge_sequences.fna" > tmp.fna && mv tmp.fna "$CENTRIFUGE_DB/centrifuge_sequences.fna"

# Count check
grep -c "^>" "$CENTRIFUGE_DB/centrifuge_sequences.fna" > "$CENTRIFUGE_DB/num_sequences.txt"

# Build Centrifuge DB
centrifuge-build \
  -p 16 \
  --conversion-table "$CONVERSION_TABLE" \
  --taxonomy-tree "$TAX_TREE" \
  --name-table "$TAX_NAMES" \
  "$CENTRIFUGE_DB/centrifuge_sequences.fna" \
  "$CENTRIFUGE_DB"

conda deactivate
